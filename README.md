# Transformers-from-scratch
 In this repository, I am building a Transformer model from scratch, covering components like self-attention, multi-head attention, layer normalization, and positional encoding, along with constructing the encoder and decoder layers. This project provides a clear breakdown of the Transformer architecture for better understanding.
